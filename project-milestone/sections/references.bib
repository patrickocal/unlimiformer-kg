@inproceedings{huguet2021rebel,
    title = "{REBEL}: Relation Extraction By End-to-end Language generation",
    author = "Huguet Cabot, Pere-Llu{\'i}s and Navigli, Roberto",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.204",
    doi = "10.18653/v1/2021.findings-emnlp.204",
    pages = "2370--2381",
    abstract = "Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model{'}s flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them.",
}


@article{kryscinski2021booksum,
    title={BookSum: A Collection of Datasets for Long-form Narrative Summarization}, 
    author={Wojciech Kry{\'s}ci{\'n}ski and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},
    year={2021},
    journal={\url{https://arxiv.org/abs/2105.08209}},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{bertsch2023unlimiformer,
  title={Unlimiformer: Long-Range Transformers with Unlimited Length Input},
  author={Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew R},
  journal={\url{https://arxiv.org/pdf/2305.01625v1.pdf}},
  year={2023}
}

@inproceedings{wang2022multi,
  title={Multi-Document Scientific Summarization from a Knowledge Graph-Centric View},
  author={Wang, Pancheng and Li, Shasha and Pang, Kunyuan and He, Liangliang and Li, Dong and Tang, Jintao and Wang, Ting},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={6222--6233},
  year={2022}
}

@inproceedings{ribeiro2017struc2vec,
  title={struc2vec: Learning node representations from structural identity},
  author={Ribeiro, Leonardo FR and Saverese, Pedro HP and Figueiredo, Daniel R},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={385--394},
  year={2017}
}


@article{wu2020extracting,
  title={Extracting summary knowledge graphs from long documents},
  author={Wu, Zeqiu and Koncel-Kedziorski, Rik and Ostendorf, Mari and Hajishirzi, Hannaneh},
  journal={\url{https://arxiv.org/pdf/2009.09162.pdf}},
  year={2020}
}

@article{huang2021efficient,
    title={Efficient Attentions for Long Document Summarization}, 
    author={Luyang Huang and Shuyang Cao and Nikolaus Parulian and Heng Ji and Lu Wang},
    year={2021},
    journal={\url{https://arxiv.org/pdf/2104.02112.pdf}},
}


@article{galkin2021nodepiece,
  author       = {Mikhail Galkin and
                  Jiapeng Wu and
                  Etienne G. Denis and
                  William L. Hamilton},
  title        = {NodePiece: Compositional and Parameter-Efficient Representations of
                  Large Knowledge Graphs},
  journal      = {CoRR},
  volume       = {abs/2106.12144},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.12144},
  eprinttype    = {arXiv},
  eprint       = {2106.12144},
  timestamp    = {Wed, 27 Apr 2022 17:47:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-12144.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
