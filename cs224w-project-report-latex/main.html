<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<center>
<h1
id="summarizing-knowledge-graph-augmented-long-documents">Summarizing
Knowledge-Graph-Augmented Long Documents</h1>
</center>
<p>By <em>Patrick O’Callaghan</em>, <em>Sheel Sansare</em>, <em>Tristan
Wang</em> as part of the Stanford CS224W Winter 2023 course project</p>
<p>Our colaboratory notebook provides some additional detail of our
codebase.</p>
<h2 id="to-do-list-remove-when-done">To-Do List (Remove When Done)</h2>
<ol type="1">
<li><p>Explain objective function we are optimizing during
training.</p></li>
<li><p>Submit test results to Scrolls.</p></li>
<li><p>Present key result that length of summary is strongly dependent
on input: LD &lt; KG + LD &lt; KG. Explain why this is.</p></li>
<li><p>Upload models to Hugging Face.</p></li>
<li><p>Figures</p></li>
<li><p>Shakespeare image</p></li>
</ol>
<ol start="8" type="1">
<li><p>Sheel’s KG.</p></li>
<li><p>Plot distribution of LD, KG, and summary sizes for the 3
splits.</p></li>
<li><p>Graph convergence of summary length (number of tokens) to 90 for
LDs, 750 for combined, 800+ for KGs. <see wandb></p></li>
<li><p>Training / loss and other graphs from the training. we need to
discuss training in more detail eg what is the loss function? or how
does the trainer work? The relevant module is <a
href="src/utils/custom_seq2seq_trainer.py"
class="uri">src/utils/custom_seq2seq_trainer.py</a> I think we need to
summarize this either in the colab or the blog. This custom trainer has
minor modifications of <a
href="https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers"
class="uri">https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers</a>
Trainer This means that it uses a standard cross-entropy loss function
... adamw optimizer</p></li>
<li><p>Table of results comparing R1, R2, RL, BERTScore F1 for the 3
experiments. Bold the best performers.</p></li>
<li><p>Acknowledge any potential weaknesses in our experimental
approach: 13.1 eg there may be an advantage to the combined model. What
would we do differently with more time: fine-tune for longer summaries
using the other two datasets (LD and KG alone). Then repeat to see if
KG+LD still wins. I don’t think this is too much of a problem as, if
there is no improvement to be made, training will quickly halt. 13.2
training involves truncating the long document to just under 16.5k
tokens. By putting the KG at the front of the text, we are truncating
more of the LD. For the longest of documents, the KGs are upto 50k
tokens long.</p></li>
</ol>
<h2 id="introduction">Introduction</h2>
<p>Long documents are often difficult to understand and summarize. This
is especially true of technical documents such as government reports
where entities are often obscure institutions or less-well-known
individuals. Literature provides one way of dealing with this form of
complexity: <em>introduce a knowledge graph at the beginning of the
text</em>. Famous examples include the works of Shakespeare where the
main text of each play is preceded by a <em>dramatis personea</em> or
<em>cast of characters</em> (and their relations).</p>
<p>&lt;photo&gt;</p>
<p>In these settings, the role of the knowledge graph is to provide a
structured and easy-to-refer-to characterisation of key entities in the
document. A more extensive example is a complicated historical text such
as Hilary Mantel’s “The mirror and the light”. There the contents page
is followed by seven pages of structured knowledge-graph-like text.</p>
<p>In this blog post, we explore how knowledge graphs (KGs) can applied
to improve the summarization of long documents. To do so, we exploit a
recent innovation in long-document summarization that allows for
documents of arbitrary length called <a
href="https://arxiv.org/abs/2305.01625">unlimiformer</a>.</p>
<h3 id="problem-statement">Problem Statement</h3>
<p>Until recently long documents were already too long for the limited
context window of attention of transformer models. Whilst the context
window is still limited, various ways to extend the context window have
emerged. A natural, yet counter-intuitive, question then arises:</p>
<center>
Will summarization improve if we <em>extend</em> or augment a document
with its knowledge graph?
</center>
<p>Our conjecture is that augmenting long documents with their knowledge
graphs will indeed help large language models generate better summaries
of long documents. Our goal is therefore to build the right datasets,
choose the right architecture and design suitable experiments that will
enable us measure any possible impact of including a “cast of entities”
(and their relations) at the beginning of the document.</p>
<h3 id="knowledge-graphs-kgs-of-long-documents-lds">Knowledge Graphs
(KGs) of Long Documents (LDs)</h3>
<p>Knowledge graphs stand in contrast with long documents in that they
are structured and concise. They form a significant reduction of the
document to facts (expressed as relations between entities).</p>
<p>We choose the REBEL end-to-end relation extractor to generate our
knowledge graphs.</p>
<h3 id="two-new-kg-datasets">Two new KG datasets</h3>
<p>In this project, we generate a new collection of knowledge graphs:
one for each example in the GovReport dataset. This is a significant
undertaking for two reasons:</p>
<ol type="1">
<li><p>there are approximately 19,500 documents in GovReport;</p></li>
<li><p>the variance in the length of documents is significant and this
leads to major hardware management issues during generation.</p></li>
</ol>
<p>There are significant design choices relating to how relations are
specified and passed to the language model to generate summaries. We
specify each KG as a single sequence of subsequences: one subsequence
for each relation triplet in the KG. We then integrate the collection of
KGs with GovReport.</p>
<p>The <a
href="https://huggingface.co/patrickocal/gov_report_kg/viewer/gov_report_kg">first
dataset</a> replaces each LD in GovReport with a KG. The <a
href="https://huggingface.co/datasets/patrickocal/gov_report_kg/viewer/gov_re%20port_kg_comb">second
dataset</a> replaces each LD with a single string that is the
concatenation of the KG and LD.</p>
<p><img src="images/input_stats.png" alt="Inputs Table" /> | | Input
Token Lengths (Train/Dev/Test) | | | | |——-|———————————————|——-|——-|———|
| | Average | Min | Max | Std Dev | | LD | (9617/10044/9209) |
(74/237/561) | (303192/69300/38735) | (7644/7106/5446) | | KG |
(2820/2902/2766) | (121/401/223) | (63988/13049/12728) |
(2013/1782/1625) | | KG+LD | (13203/13854/12829) | (487/1541/825) |
(313947/77692/58815) | (9753/9065/7525) |</p>
<h3 id="training-bartunlimiformer">Training BART+Unlimiformer</h3>
<p><a href="__">Unlimiformer</a> is a recent retrieval-based method for
augmenting LLMs at the decoder level, is the first long-range
transformer to support unlimited length inputs. The key innovation of
unlimiformer is to create a datastore of encodings which correspond to
each token in the original document, and use the <span
class="math inline"><em>k</em></span>-nearest neighbors (<span
class="math inline"><em>k</em></span>-NN) algorithm to select the <span
class="math inline"><em>k</em></span> most relevant tokens in the
datastore during decoding.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> search(<span class="va">self</span>, queries, k):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">        -search method retrieves the indices of the k closest vectors to query</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(queries.shape) <span class="op">==</span> <span class="dv">1</span>: <span class="co"># searching for only 1 vector, add one extra dim</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logger.info(<span class="st">&quot;Searching for a single vector; unsqueezing&quot;</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>            queries <span class="op">=</span> queries.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> queries.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.dimension <span class="co"># query vectors are same shape as &quot;key&quot; vectors</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_flat_index:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.gpu_index:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                scores, values <span class="op">=</span> faiss.knn_gpu(faiss.StandardGpuResources(),</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                                               queries,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                                               <span class="va">self</span>.keys,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                                               k,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                                               metric<span class="op">=</span>faiss.METRIC_INNER_PRODUCT,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>                                               device<span class="op">=</span><span class="va">self</span>.device.index</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>                                               )</span></code></pre></div>
<h3 id="our-experiments">Our experiments</h3>
<p>Our experiments focus on comparing the summary outputs across the
three datasets: the original GovReports, the GovReportsKG and the
GovReportsKG+LD. Our initial findings reveal significant differences
between the summaries generated from LDs vs the new datasets. The
default BART model produces summaries of approximately 130 tokens with a
typical range of 100 to and 150 tokens. In contrast, the KGs and KG+LDs
generated summaries of approximately 900 tokens with a typical range of
600 to 1100. The target/golden summaries for GovReport are closer to the
latter with the number of tokens being 600 on average with a typical
range of between 400 and 1000.</p>
<figure>
<img src="images/initial_results_stats.png"
alt="Initial Results Table" />
<figcaption aria-hidden="true">Initial Results Table</figcaption>
</figure>
<p>Initial Result Summary Token Lengths (Validation) | | Average | Min |
Max | Std Dev | |——–|————————————————–|—–|——|——–| | LD | 128 | 86 | 130
| 2 | | KG | 737 | 494 | 1022 | 65 | | KG+LD | 755 | 500 | 916 | 52
|</p>
<p>We explore the cause of these differences and refine our experiments
to control for length of summary. We do so by re-initializing training
with a model that is fine-tuned to produce longer summaries. The goal is
to create a fair “horse race” to compare summarization performance
across the three datasets.</p>
<h3 id="overview-of-our-final-results">Overview of our final
results</h3>
<p>Once we control for length of summary, our final results are in line
with our initial hypothesis. We summarise these results in <a
href="#fig:summary-of-results-intro" data-reference-type="ref"
data-reference="fig:summary-of-results-intro">[fig:summary-of-results-intro]</a>.</p>
<figure>
<img src="images/final_results_stats.png" alt="Final Results Table" />
<figcaption aria-hidden="true">Final Results Table</figcaption>
</figure>
<p>Final Result Summary Token Lengths (Validation) | | Average | Min |
Max | Std Dev| |——–|————————————————|——|——|——–| | LD | 943 | 630 | 1026
| 80 | | KG | 555 | 364 | 824 | 66 | | KG+LD | 657 | 476 | 1023 | 109
|</p>
<figure>
<img src="images/results_table.png" alt="Results Table" />
<figcaption aria-hidden="true">Results Table</figcaption>
</figure>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 17%" />
<col style="width: 39%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>Base Model</th>
<th>Input Type</th>
<th>ROUGE 1/2/L/GeoMean</th>
<th>BERTScore F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BARTbase</td>
<td>LD (Test Set)</td>
<td>(56.6/26.3/27.6/—–)</td>
<td>0.682</td>
</tr>
<tr class="even">
<td>BARTbase+18k</td>
<td>LD</td>
<td>(50.1/20.9/21.5/28.2)</td>
<td>0.639</td>
</tr>
<tr class="odd">
<td>BARTbase+18k</td>
<td>KG</td>
<td>(44.0/13.8/19.5/22.8)</td>
<td>0.609</td>
</tr>
<tr class="even">
<td>BARTbase+18k</td>
<td>KG+LD</td>
<td>(53.2/22.5/23.6/30.5)</td>
<td>0.655</td>
</tr>
</tbody>
</table>
<p>We find that the best summaries are indeed produced by the combined
KG+LD input. This is followed by LDs and then finally KGs. There is a
significant difference in performance between the three. All our results
are for the validation set.</p>
<h2 id="methodology">Methodology</h2>
<p>We use</p>
<p>We compare and contrast the LD summaries generated by 3
transformer-based LLM models. Firstly, we train the facebook/BART base
model using the unlimiformer augmentation, which operates on the entire
LD and employs the <span class="math inline"><em>k</em></span>-NN
algorithm. Secondly, we repeat the previous exercise but with KGs as
inputs instead of LDs. Thirdly, we repeat the previous exercise with
string inputs of concatenated KGs and LDs (in this order).</p>
<h3 id="specifics-of-generating-each-kg">Specifics of generating each
KG</h3>
<p>Our baseline dataset is the Hugging Face version of GovReport <span
class="citation"
data-cites="huang2021efficient">[@huang2021efficient]</span>, a
well-established LD summarization dataset with many practical
applications. To generate the required datasets, we use <a
href="**paper%20url**">REBEL</a>, a pre-trained model that can be found
on <a href="%5E2">Hugging Face</a>, to perform one-shot named-entity
recognition (NER) and relation extraction (RE). This end-to-end approach
stands in contrast to the traditional two-step approach (eg. Spacy and
Stanford …) We explored some of the alternatives to REBEL and discuss
those in more detail in the <a href="#appendix">appendix</a></p>
<h5 id="fix-refs">(fix refs ^)</h5>
<h4 id="rebel">REBEL</h4>
<p>We choose REBEL because at the time of writing it is the <a
href="link">top performing</a> end-to-end relation extractor on the <a
href="DocRED%20paper%20by%20Yao%20et%20al%20%5B@yao2019DocRED%5D">DocRED
dataset</a>. Moreover, it is more straightforward and fast to implement
and produced better KGs than the alternatives we tried. Moreover, with
more time to develop our framework, we believe that REBEL would be
well-suited to integration with Unlimiformer to generating KGs at
inference time. (The LD needs to be split into chunks for both KG and
summarization, and Unlimiformer is designed to capture hidden encodings
of inputs, so an integration of this form would be very natural. We
leave this extension to future work.)</p>
<p>Given the time and compute resources available to us, through trial
and error, we found that extracting three or four head-relation-tail
triplets per 128-token chunk is optimal. We set the span_length,
<strong>parameter</strong>, <code>num_beams</code> parameters to control
extraction. Recall that <code>num_beams</code> is the maximum number of
candidates (in this case relation triplets) that the attention mechanism
will hold over the <code>span_length</code> of text, which is in this
case 128 tokens or approximately the length of a single paragraph.</p>
<p>[code example here]</p>
<p><strong>put the following comment the figure caption or colab (it’s
more a comment for marks than a general interest)</strong> Once the
triplets are extracted, we use NetworkX to create a directed graph, and
MatPlotLib to visualize and plot the results. Below is a sample image of
a knowledge graph produced from a gold summary.</p>
<figure>
<img src="images/kg.png" alt="KG Plot" />
<figcaption aria-hidden="true">KG Plot</figcaption>
</figure>
<h4 id="bart-specific-knowledge-graph-representation">BART-specific
Knowledge Graph representation</h4>
<p>We chose to use the beginning of sequence (BOS, ‘&lt;s&gt;’) and end
of sequence (EOS, ‘&lt;/s&gt;’) tokens to separate triples in our
knowledge graphs (KGs) with the intent of aligning with BART’s
understanding of sequence boundaries, this approach has specific
implications:</p>
<ol type="1">
<li><p><strong>Clear Segmentation of Information</strong>: Using BOS and
EOS tokens to delimit triples in the KG makes each triple a distinct
segment from the model’s perspective. This is beneficial since we want
the model to treat each triple as an independent unit of
information.</p></li>
<li><p><strong>Facilitating Attention Across Segments</strong>: This
segmentation should help the model’s attention mechanism focus on each
triple individually, potentially enhancing the model’s ability to
capture the nuances of each relationship within the KG.</p></li>
</ol>
<ol start="4" type="1">
<li><strong>Potential for Contextual Integration</strong>: While each
triple is treated as a separate sequence, the overall
sequence-of-subsequences structure still allows BART to integrate these
segments contextually. The model can still learn to understand the KG as
a whole, even though it processes each triple individually.</li>
</ol>
<h3 id="specifics-of-augmenting-the-govreport-dataset">Specifics of
augmenting the GovReport dataset</h3>
<h4 id="govreport">GovReport</h4>
<p>The GovReport dataset is a well-established long-document
summarization datasets that is both publicly available and ready-to-use.
We use it because it is a large and popluar dataset that has many
real-world applications. The Hugging Face GovReport <a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
dataset has an approximate <span class="math inline">90/5/5%</span>
split of approximately <span class="math inline">19.5</span>k
document-summary pairs.</p>
<h2 id="training">Training</h2>
<h3 id="unlimiformer">Unlimiformer</h3>
<p>**Why unlimiformer, and what is it?** Augmenting large language
models (LLMs) to handle long documents using retrieval-based methods is
a highly active area of research. Since Vaswani et al 2017, transformers
have become the default approach to natural language processing.
Transformers have succeeded due to their ability to capture long range
dependencies between tokens. They do so by abandoning the sequential
approach of recurrent neural networks and instead allowing the decoder
to attend to a complete graph over the encoded hidden states of tokens.
The complexity of complete graphs is therefore quadratic in the number
of tokens. The result is a powerful <em>attention</em> mechanism, but
one that is local and restricted to the <em>context window</em>. The
context window of ChatGPT-3.5 is 4,096 tokens, while the average novel
contains well over 100,000 tokens. Proprietory models such as GPT-4 and
Claude provide users with models that extend beyond 100,000 tokens, but
the question remains: what is the best way to achieve this?</p>
<h4 id="retrieval-augmentations-of-llms">Retrieval-Augmentations of
LLMs</h4>
<p>Unlimiformer stands out for its novel integration of retrieval
mechanisms directly into the Transformer architecture. This integration
allows the model to dynamically access large-scale, a document-specific
external (FAISS) datastore during inference. This datastore is populated
with encoded representations of the full input text. The key advantage
of this approach is that it enables the model to augment its language
generation capabilities with contextually relevant, externally stored
information. This is useful for tasks requiring deep, specific knowledge
or for improving the model’s ability to stay updated with recent
information.</p>
<h4 id="comparison-with-other-methods-datastore-access">Comparison with
Other Methods (Datastore Access)</h4>
<p>Unlike traditional methods where datastores are accessed externally
or through separate mechanisms, Unlimiformer integrates the datastore
access internally within its architecture. This internal integration
facilitates a more seamless and efficient interaction between the
model’s language processing capabilities and the external knowledge
sources. In contrast, other models might rely on separate retrieval
steps or external systems to incorporate knowledge from datastores,
which can introduce complexity and inefficiency. Unlimiformer’s
approach, therefore, represents a significant advancement in making
retrieval-augmented models more streamlined and effective.</p>
<p>This highlights Unlimiformer’s innovative approach to enhancing LLMs
with retrieval-augmented capabilities, particularly its unique internal
mechanism for accessing and integrating external datastores.</p>
<h3 id="bart">BART</h3>
<p>We focused on training the <code>facebook/bart-base</code> model
(henceforth BART). Although there now many more advanced models, and
many of these (e.g. Llama) are compatible with Unlimiforemer, BART
provides the main benchmark in the unlimiformer paper <span
class="citation"
data-cites="bertsch2023unlimiformer">[@bertsch2023unlimiformer]</span>.
It has a context window of 1024 tokens, anIn addition, each model treats
special tokens slightly differently and, as we shall see, the way tokens
are treated is important to the resulting training on KGs.</p>
<p>BART, like other transformer-based models, is considered adept at
handling structured inputs due to several key features of its
architecture and design. <em>Structured inputs</em> refer to data that
is organized in a predictable, often hierarchical manner, with clear
relationships between different parts. This contrasts with unstructured
data, like free-form text, where the organization and relationships are
not as explicitly defined. Examples of structured inputs include:
databases or tables; XML or JSON data, where elements are nested and
have defined relationships; Knowledge graphs, where information is
represented as entities and relationships (triples).</p>
<h3 id="how-we-use-bart-for-training">How we use BART for training</h3>
<h3 id="appropriateness-of-the-bart-model">Appropriateness of the BART
Model</h3>
<p>When training our model, we chose to feed the relational data of our
KGs as tokens into unlimiformer, as opposed to embedding the KGs as
separate relations into vector space. We believe that our approach is
more appropriate as it allows us to better utilize the the unlimiformer
framework, while preserving as much of the KG structure as possible
within the dataset.</p>
<p>**Work to train models individually.**</p>
<h4 id="background-on-bart">Background on BART</h4>
<p>**Structured Inputs</p>
<p>**Why BART Handles Structured Inputs Well**</p>
<p>1. **Self-Attention Mechanism**: BART’s transformer architecture uses
a self-attention mechanism, which allows it to consider the entire input
sequence at once. This enables the model to understand relationships
between different parts of the input, essential for structured data.</p>
<p>2. **Contextual Understanding**: BART can capture context from both
left and right of each token in the input sequence. This bi-directional
context is crucial for understanding structured inputs, where the
meaning often depends on the surrounding elements.</p>
<p>3. **Layered Encoding**: The layered structure of transformers
enables them to capture and encode different levels of abstraction,
which is beneficial for understanding hierarchical and nested structures
in the input.</p>
<p>4. **Pre-training on Diverse Data**: BART is pre-trained on a wide
range of data, including structured formats. This pre-training helps it
to learn patterns and structures that are common in various types of
data.</p>
<p>5. **Flexibility in Input Representation**: BART can handle sequences
with special tokens and delimiters, allowing it to adapt to different
types of structured inputs. For example, it can process inputs where
parts of the data are segmented or highlighted using special tokens.</p>
<p>6. **Adaptability to Task-Specific Structures**: With fine-tuning,
BART can adapt to specific types of structured inputs relevant to a
particular task, enhancing its ability to process and generate
meaningful outputs based on that structure.</p>
<p>In summary, BART’s ability to process and understand the entire input
sequence contextually, along with its adaptability and pre-training on
diverse data, makes it well-suited for handling structured inputs. This
capability allows it to effectively process and generate outputs based
on inputs like knowledge graphs.</p>
<p>In this context, the slower training times you observed might not be
due to the tokenization strategy per se but could involve other factors
such as the complexity of the relationships in the KGs, the adaptation
of the model to this unique structuring of inputs, or other
computational aspects related to how the BART model processes these
inputs.</p>
<p>Your approach aligns with the design principles of transformer models
like BART, which are adept at handling structured inputs. The key would
be to ensure that the rest of your training pipeline, including data
preprocessing and model fine-tuning, is optimized to leverage this
structure effectively.</p>
<h3 id="metrics">Metrics</h3>
<p>When it comes to summarizing long documents, metrics like ROUGE and
BertScore are critical. They offer a standardized, quantitative way to
assess the performance of summarization algorithms.</p>
<h4 id="rouge-metrics">ROUGE Metrics</h4>
<ul>
<li><strong>ROUGE-1</strong> measures the overlap of unigrams
(individual words) between the system-generated summary and a set of
reference summaries. It captures the surface-level accuracy, essentially
checking if the key terms appear in the summary.</li>
<li><strong>ROUGE-2</strong> measures the overlap of unigrams
(individual words) between the system-generated summary and a set of
reference summaries. It captures the surface-level accuracy, essentially
checking if the key terms appear in the summary.</li>
<li><strong>ROUGE-L</strong> focuses on the longest common subsequence
between the generated summary and the reference. It can recognize longer
phrases that appear in both texts, which reflects a higher level of
semantic similarity.</li>
<li><strong>ROUGE Geometric Mean</strong> is a composite score that
combines ROUGE-1, ROUGE-2, and ROUGE-L (sometimes including others, like
ROUGE-SU4) by calculating their geometric mean. It balances the
contribution of each metric, offering a more holistic view of the
quality of the summary.</li>
</ul>
<h4 id="bertscore">BertScore</h4>
<ul>
<li><strong>BertScore F1</strong> leverages the power of BERT. BertScore
computes the similarity of each token in the candidate summary to each
token in the reference summary and vice versa, resulting in precision
and recall scores. The F1 score is the harmonic mean of these two,
providing a balance that considers both the summary’s coverage of
content and the content’s relevance to the summary.</li>
</ul>
<h2 id="results">Results</h2>
<p>**How did our model perform compared to the baseline?
Explanation?**</p>
<p>**Why is the average summary 800 words and not 500 words?**</p>
<p>Interpreting the performance differences between models trained on
long documents (LD) and knowledge graphs (KG) based on the provided
metrics involves considering what each metric measures and how that
relates to the nature of the inputs:</p>
<p>1. **ROUGE Scores**: - **ROUGE-1 (LD: 23, KG: 40)**: This measures
the overlap of unigrams (individual words) between the generated summary
and the reference summary. The substantially higher score for KG
suggests that the KG-based model is better at capturing key content
words. This could be because KGs, being structured and concise, might
enable the model to focus on essential terms more effectively. -
**ROUGE-2 (LD: 11.74, KG: 11.47)**: This metric evaluates bigram
overlap, indicating how well the model captures phrases and specific
content. The similar scores suggest that both models are nearly equally
effective at capturing phrase-level information, though the LD model has
a slight edge. - **ROUGE-L (LD: 14.7, KG: 17.7)**: ROUGE-L assesses the
longest common subsequence, which reflects sentence-level structure and
coherence. The higher score for KG indicates better preservation of
sentence structure or flow from the KG inputs.</p>
<p>2. **BERTScore**: - **Precision (LD: 0.69, KG: 0.58)**: Precision
measures how much of the content in the generated summary is relevant or
present in the reference summary. The higher precision for LD implies
that it might be better at generating content closely aligned with the
reference, likely due to the richer context provided by the long
document. - **Recall (LD: 0.52, KG: 0.57)**: Recall assesses how much of
the reference summary is captured in the generated summary. The higher
recall for KG suggests it is better at including essential points from
the reference summary, possibly due to the distilled and focused nature
of KGs. - **F1/Aggregated BERTScore (LD: 0.59, KG: 0.57)**: This
balanced metric considers both precision and recall. The scores are
quite close, indicating that overall, both models are similarly
effective in terms of content relevance and coverage, though the LD
model has a marginal advantage.</p>
<p>Given these results after 8000 training steps:</p>
<p>- The KG-based model excels in capturing key content words and
maintaining sentence structure, likely due to the concise and structured
nature of KGs. - The LD-based model, despite its lower performance in
some ROUGE metrics, achieves higher precision in BERTScore, suggesting
its strength in generating summaries that are more aligned with the
content and style of the reference summaries. This might be due to the
rich, contextual information available in long documents. - The marginal
differences in ROUGE-2 and BERTScore/F1 indicate that both models have
their strengths and weaknesses. The LD model seems to be better for
generating precise content, while the KG model is more effective in
covering essential information and maintaining structure.</p>
<p>In conclusion, each model has its advantages depending on the desired
outcome of the summary: whether the focus is on precise content
alignment (LD) or on covering key points and maintaining structure (KG).
Continued training and further optimization could enhance the
performance of both models, potentially narrowing these gaps.</p>
<h2 id="conclusion">Conclusion</h2>
<p>**Do we recommend using KGs for LD summarization?**</p>
<h2 id="appendix">Appendix</h2>
<h3 id="alternatives-to-rebel">Alternatives to REBEL</h3>
<p>Other means of performing NER and RE we considered include spaCy-LLM,
DyGIE++, and LlamaIndex. spaCy-LLM<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a> is a package that
integrates LLMs into natural language processing (NLP) pipelines
provided by spaCy, an industry-standard NLP library. In particular, its
built-in <code>spacy.REL.v1</code><a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a> component supports RE
with both zero-shot and few-shot prompting, but relies on an upstream
NER component for entity extraction.</p>
<p>DyGIE++ is an RE component that refines and scores text spans
designed to capture both intra-sentence and cross-sentence context. We
cloned the code from the official GitHub repository linked here<a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a> and attempted to replicate the
process of training a model for RE, but were unsuccessful due to
technical difficulties.</p>
<p>Finally, LlamaIndex, a framework for connecting data sources for
LLMs, has a class called <code>KnowledgeGraphIndex</code><a href="#fn5"
class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>
which is compatible with FAISS, the datastore that unlimiformer uses to
conduct <span class="math inline"><em>k</em></span>-NN searches of
top-level hidden state encodings, which would simplify our task of NER
and RE.</p>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li
id="fn1"><p>https://huggingface.co/datasets/ccdv/govreport-summarization<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://spacy.io/usage/large-language-models<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn3"><p>https://github.com/explosion/spacy-llm/tree/main/usage_examples/rel_openai<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://github.com/dwadden/dygiepp<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn5"><p>https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body>
</html>
