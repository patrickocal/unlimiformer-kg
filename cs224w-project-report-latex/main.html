<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">
<p><strong>Project Report</strong><br />
<strong>Long Document Summarization:</strong><br />
<strong>Augmenting Unlimiformer with Knowledge Graphs<a href="#fn1"
class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></strong><br />
</p>
<div id="tab:my_label">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;">Patrick O’Callaghan</td>
<td style="text-align: center;">Sheel Sansare</td>
<td style="text-align: center;">Tristan Wang</td>
</tr>
<tr class="even">
<td style="text-align: center;">(patocal)</td>
<td style="text-align: center;">(ssansa2)</td>
<td style="text-align: center;">(aawang99)</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:my_label" label="tab:my_label"></span></p>
</div>
<h2 class="unnumbered" id="to-do-list-remove-when-done">To-Do List
(Remove When Done)</h2>
<p>1. Explain objective function we are optimizing during training.</p>
<p>2. Submit test results to Scrolls.</p>
<p>3. Email Tolu on how to present code that doesn’t run on Colab.</p>
<p>4. Present key result that length of summary is strongly dependent on
input: LD &lt; KG + LD &lt; KG. Explain why this is.</p>
<p>5. Upload models to Hugging Face.</p>
<p>6. Figures</p>
<p>6.1 Shakespeare image with KG / dramatis personae.</p>
<p>6.2 The Mirror and the Light (Hilary Mantel).</p>
<p>6.3 Sheel’s KG.</p>
<p>6.4 Plot distribution of LD, KG, and summary sizes for the 3
splits.</p>
<p>6.5 Graph convergence of summary length (number of tokens) to 90 for
LDs, 750 for combined, 800+ for KGs.</p>
<p>6.6 Training / loss and other graphs from the training.</p>
<p>6.7 Table of results comparing R1, R2, RL, BERTScore F1 for the 3
experiments. Bold the best performers.</p>
<h1 id="introduction">Introduction</h1>
<p>In this blog post, we explore how knowledge graphs (KGs) can be
applied to improve the accuracy of the <code>unlimiformer</code>
long-range transformer for the task of long document (LD)
summarization.</p>
<h1 id="problem-statement">Problem Statement</h1>
<p>Augmenting large language models (LLMs) to handle long documents
using retrieval-based methods is a highly active area of research. The
key weakness of modern transformer-based LLMs is the token limit, or
context window of attention. For instance, the context window of
ChatGPT-3.5 is 4,096 tokens, while the average novel contains well over
100,000 tokens.</p>
<p><code>Unlimiformer</code> <span class="citation"
data-cites="bertsch2023unlimiformer">[@bertsch2023unlimiformer]</span>,
a recent retrieval-based method for augmenting LLMs at the decoder
level, is the first long-range transformer to support unlimited length
inputs. The key innovation of <code>unlimiformer</code> is to create a
datastore of encodings which correspond to each token in the original
document, and use the <span
class="math inline"><em>k</em></span>-nearest neighbors (<span
class="math inline"><em>k</em></span>-NN) algorithm to select the <span
class="math inline"><em>k</em></span> most relevant tokens in the
datastore during decoding.</p>
<p>Since KGs store richer information than plain datastores, we predict
that feeding KG relational data as tokens into <code>unlimiformer</code>
can enhance the model’s understanding of LDs, like how a <em>dramatis
personae</em> section can aid a reader in understanding a complicated
novel by highlighting key relationships between characters. Thus, we
hypothesize that the KG-augmented <code>unlimiformer</code> model will
produce more accurate LD summaries than the baseline
<code>unlimiformer</code> model.</p>
<h1 id="methodology">Methodology</h1>
<p>We compare and contrast the LD summaries generated by 3
transformer-based LLM models. Firstly, we train the facebook/BART base
model using the <code>unlimiformer</code> augmentation, which operates
on the entire LD and employs the <span
class="math inline"><em>k</em></span>-NN algorithm. Secondly, we repeat
the previous exercise but with KGs as inputs instead of LDs. Thirdly, we
repeat the previous exercise with string inputs of concatenated KGs and
LDs (in this order).</p>
<h1 id="creating-the-kgs-and-datasets">Creating the KGs and
Datasets</h1>
<p>Our baseline dataset is the Hugging Face version of GovReport <span
class="citation"
data-cites="huang2021efficient">[@huang2021efficient]</span>, a
well-established LD summarization dataset with many practical
applications. To generate the required datasets, we use REBEL <span
class="citation" data-cites="huguet2021rebel">[@huguet2021rebel]</span>,
a pre-trained, end-to-end relation extraction model that can be found on
Hugging Face here<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>, to perform named-entity recognition
(NER) and relation extraction (RE).</p>
<h2 class="unnumbered" id="govreport">GovReport</h2>
<p>**Why GovReport, and how is it used?**</p>
<h2 class="unnumbered" id="rebel">REBEL</h2>
<p>We chose REBEL because it is end-to-end (it finds entities and
relations at the same time), open-source, and easy to implement using
Hugging Face. Also, according to the DocRED paper by Yao et al <span
class="citation" data-cites="yao2019DocRED">[@yao2019DocRED]</span>,
pretrained REBEL yields the best joint entity and relation extraction
(NER and RE) result compared with the benchmark among all models
sampled, with a relation F1 score of 47.1<a href="#fn3"
class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>.</p>
<p>Since the pre-trained REBEL model has a token limit, we chose to
split the LD into 128-token chunks before extracting head-relation-tail
triplets from each chunk. Next, we used NetworkX to assemble the
triplets and form a directed KG. The code for our REBEL implementation
can be found here<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<p>**Why 128 span length?**</p>
<p>**Why extract triplets (and not extract triplets typed)?**</p>
<h2 class="unnumbered" id="alternatives-to-rebel">Alternatives to
REBEL</h2>
<p>Other means of performing NER and RE we considered include spaCy-LLM,
DyGIE++, and LlamaIndex. spaCy-LLM<a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a> is a package that
integrates LLMs into natural language processing (NLP) pipelines
provided by spaCy, an industry-standard NLP library. In particular, its
built-in <code>spacy.REL.v1</code><a href="#fn6" class="footnote-ref"
id="fnref6" role="doc-noteref"><sup>6</sup></a> component supports RE
with both zero-shot and few-shot prompting, but relies on an upstream
NER component for entity extraction.</p>
<p>DyGIE++ is an RE component that refines and scores text spans
designed to capture both intra-sentence and cross-sentence context. We
cloned the code from the official GitHub repository linked here<a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> and attempted to replicate the
process of training a model for RE, but were unsuccessful due to
technical difficulties.</p>
<p>Finally, LlamaIndex, a framework for connecting data sources for
LLMs, has a class called <code>KnowledgeGraphIndex</code><a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>
which is compatible with FAISS, the datastore that
<code>unlimiformer</code> uses to conduct <span
class="math inline"><em>k</em></span>-NN searches of top-level hidden
state encodings, which would simplify our task of NER and RE.</p>
<h1 id="training">Training</h1>
<h2 class="unnumbered" id="unlimiformer">Unlimiformer</h2>
<p>**Why unlimiformer, and what is it?**</p>
<h2 class="unnumbered" id="bart">BART</h2>
<p>**How do we use BART for training?**</p>
<p>**Work to train models individually.**</p>
<h3 id="background-on-bart">Background on BART</h3>
<p>BART, like other transformer-based models, is considered adept at
handling structured inputs due to several key features of its
architecture and design.</p>
<p>**Structured Inputs</p>
<p>Structured inputs refer to data that is organized in a predictable,
often hierarchical manner, with clear relationships between different
parts. This contrasts with unstructured data, like free-form text, where
the organization and relationships are not as explicitly defined.
Examples of structured inputs include:</p>
<p>- Databases or tables where data is organized in rows and columns. -
XML or JSON data, where elements are nested and have defined
relationships. - Knowledge graphs, where information is represented as
entities and relationships (triples).</p>
<p>**Why BART Handles Structured Inputs Well**</p>
<p>1. **Self-Attention Mechanism**: BART’s transformer architecture uses
a self-attention mechanism, which allows it to consider the entire input
sequence at once. This enables the model to understand relationships
between different parts of the input, essential for structured data.</p>
<p>2. **Contextual Understanding**: BART can capture context from both
left and right of each token in the input sequence. This bi-directional
context is crucial for understanding structured inputs, where the
meaning often depends on the surrounding elements.</p>
<p>3. **Layered Encoding**: The layered structure of transformers
enables them to capture and encode different levels of abstraction,
which is beneficial for understanding hierarchical and nested structures
in the input.</p>
<p>4. **Pre-training on Diverse Data**: BART is pre-trained on a wide
range of data, including structured formats. This pre-training helps it
to learn patterns and structures that are common in various types of
data.</p>
<p>5. **Flexibility in Input Representation**: BART can handle sequences
with special tokens and delimiters, allowing it to adapt to different
types of structured inputs. For example, it can process inputs where
parts of the data are segmented or highlighted using special tokens.</p>
<p>6. **Adaptability to Task-Specific Structures**: With fine-tuning,
BART can adapt to specific types of structured inputs relevant to a
particular task, enhancing its ability to process and generate
meaningful outputs based on that structure.</p>
<p>In summary, BART’s ability to process and understand the entire input
sequence contextually, along with its adaptability and pre-training on
diverse data, makes it well-suited for handling structured inputs. This
capability allows it to effectively process and generate outputs based
on inputs like knowledge graphs, tables, or other structured data forms.
We chose to use the beginning of sequence (BOS, ‘&lt;s&gt;’) and end of
sequence (EOS, ‘&lt;/s&gt;’) tokens to separate triples in our knowledge
graphs (KGs) with the intent of aligning BART’s understanding of
sequence boundaries, this approach has specific implications:</p>
<p>1. **Clear Segmentation of Information**: Using BOS and EOS tokens to
delimit triples in the KG makes each triple a distinct segment from the
model’s perspective. This is beneficial since we want the model to treat
each triple as an independent unit of information since we expect our
GovReport KGs to be such that the relationships within triples contain
key information.</p>
<p>2. **Facilitating Attention Across Segments**: This segmentation
should help the model’s attention mechanism focus on each triple
individually, potentially enhancing the model’s ability to capture the
nuances of each relationship within the KG.</p>
<p>3. **Model Adaptation to Structured Inputs**: Given that BART is
designed to handle structured text, using BOS and EOS tokens in this way
could aid the model in better understanding and generating summaries
based on the structured nature of KGs. It aligns with the model’s
pre-existing mechanisms for processing text.</p>
<p>4. **Potential for Contextual Integration**: While each triple is
treated as a separate sequence, the overall structure still allows the
model to integrate these segments contextually. The model can learn to
understand the KG as a whole, even though it processes each triple
individually.</p>
<p>5. **Efficient Processing of Smaller Units**: By breaking down the KG
into smaller segments, the model might process each unit more
efficiently, especially if the triples are concise and the relationships
within them are straightforward.</p>
<p>In this context, the slower training times you observed might not be
due to the tokenization strategy per se but could involve other factors
such as the complexity of the relationships in the KGs, the adaptation
of the model to this unique structuring of inputs, or other
computational aspects related to how the BART model processes these
inputs.</p>
<p>Your approach aligns with the design principles of transformer models
like BART, which are adept at handling structured inputs. The key would
be to ensure that the rest of your training pipeline, including data
preprocessing and model fine-tuning, is optimized to leverage this
structure effectively.</p>
<h2 class="unnumbered"
id="appropriateness-of-the-bart-model">Appropriateness of the BART
Model</h2>
<p>When training our model, we chose to feed the relational data of our
KGs as tokens into <code>unlimiformer</code>, as opposed to embedding
the KGs as separate relations into vector space. We believe that our
approach is more appropriate as it allows us to better utilize the the
<code>unlimiformer</code> framework, while preserving as much of the KG
structure as possible within the dataset.</p>
<h1 id="results">Results</h1>
<p>**How did our model perform compared to the baseline?
Explanation?**</p>
<p>**Why is the average summary 800 words and not 500 words?**</p>
<p>Interpreting the performance differences between models trained on
long documents (LD) and knowledge graphs (KG) based on the provided
metrics involves considering what each metric measures and how that
relates to the nature of the inputs:</p>
<p>1. **ROUGE Scores**: - **ROUGE-1 (LD: 23, KG: 40)**: This measures
the overlap of unigrams (individual words) between the generated summary
and the reference summary. The substantially higher score for KG
suggests that the KG-based model is better at capturing key content
words. This could be because KGs, being structured and concise, might
enable the model to focus on essential terms more effectively. -
**ROUGE-2 (LD: 11.74, KG: 11.47)**: This metric evaluates bigram
overlap, indicating how well the model captures phrases and specific
content. The similar scores suggest that both models are nearly equally
effective at capturing phrase-level information, though the LD model has
a slight edge. - **ROUGE-L (LD: 14.7, KG: 17.7)**: ROUGE-L assesses the
longest common subsequence, which reflects sentence-level structure and
coherence. The higher score for KG indicates better preservation of
sentence structure or flow from the KG inputs.</p>
<p>2. **BERTScore**: - **Precision (LD: 0.69, KG: 0.58)**: Precision
measures how much of the content in the generated summary is relevant or
present in the reference summary. The higher precision for LD implies
that it might be better at generating content closely aligned with the
reference, likely due to the richer context provided by the long
document. - **Recall (LD: 0.52, KG: 0.57)**: Recall assesses how much of
the reference summary is captured in the generated summary. The higher
recall for KG suggests it is better at including essential points from
the reference summary, possibly due to the distilled and focused nature
of KGs. - **F1/Aggregated BERTScore (LD: 0.59, KG: 0.57)**: This
balanced metric considers both precision and recall. The scores are
quite close, indicating that overall, both models are similarly
effective in terms of content relevance and coverage, though the LD
model has a marginal advantage.</p>
<p>Given these results after 8000 training steps:</p>
<p>- The KG-based model excels in capturing key content words and
maintaining sentence structure, likely due to the concise and structured
nature of KGs. - The LD-based model, despite its lower performance in
some ROUGE metrics, achieves higher precision in BERTScore, suggesting
its strength in generating summaries that are more aligned with the
content and style of the reference summaries. This might be due to the
rich, contextual information available in long documents. - The marginal
differences in ROUGE-2 and BERTScore/F1 indicate that both models have
their strengths and weaknesses. The LD model seems to be better for
generating precise content, while the KG model is more effective in
covering essential information and maintaining structure.</p>
<p>In conclusion, each model has its advantages depending on the desired
outcome of the summary: whether the focus is on precise content
alignment (LD) or on covering key points and maintaining structure (KG).
Continued training and further optimization could enhance the
performance of both models, potentially narrowing these gaps.</p>
<h1 id="conclusion">Conclusion</h1>
<p>**Do we recommend using KGs for LD summarization?**</p>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>2023-12-11<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://huggingface.co/Babelscape/rebel-large<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn3"><p>https://paperswithcode.com/sota/joint-entity-and-relation-extraction-on-3<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn4"><p>https://github.com/patrickocal/unlimiformer/blob/main/rebel.py<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://spacy.io/usage/large-language-models<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn6"><p>https://github.com/explosion/spacy-llm/tree/main/usage_examples/rel_openai<a
href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>https://github.com/dwadden/dygiepp<a href="#fnref7"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li
id="fn8"><p>https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html<a
href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body>
</html>
