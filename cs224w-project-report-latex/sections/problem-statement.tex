Augmenting large language models (LLMs) to handle long documents using retrieval-based methods is a highly active area of research. The key weakness of modern transformer-based LLMs is the token limit, or context window of attention. For instance, the context window of ChatGPT-3.5 is 4,096 tokens, while the average novel contains well over 100,000 tokens.

\texttt{Unlimiformer} \cite{bertsch2023unlimiformer}, a recent retrieval-based method for augmenting LLMs at the decoder level, is the first long-range transformer to support unlimited length inputs. The key innovation of \texttt{unlimiformer} is to create a datastore of encodings which correspond to each token in the original document, and use the $k$-nearest neighbors ($k$-NN) algorithm to select the $k$ most relevant tokens in the datastore during decoding.

Since KGs store richer information than plain datastores, we predict that feeding KG relational data as tokens into \texttt{unlimiformer} can enhance the model's understanding of LDs, like how a \textit{dramatis personae} section can aid a reader in understanding a complicated novel by highlighting key relationships between characters. Thus, we hypothesize that the KG-augmented \texttt{unlimiformer} model will produce more accurate LD summaries than the baseline \texttt{unlimiformer} model.