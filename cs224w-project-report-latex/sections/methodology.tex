We compare and contrast the LD summaries generated by 3 transformer-based LLM models. Firstly, we train the facebook/BART base model using the \texttt{unlimiformer} augmentation, which operates on the entire LD and employs the $k$-NN algorithm. Secondly, we repeat the previous exercise but with KGs as inputs instead of LDs. Thirdly, we repeat the previous exercise with string inputs of concatenated KGs and LDs (in this order).