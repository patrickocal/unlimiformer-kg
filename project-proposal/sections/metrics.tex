\section*{Metrics}
We will use ROUGE-1 (unigram), ROUGE-2 (bigram), ROUGE-L (sub-sequence), and BERTScore. The ROUGE metrics are a standard way of comparing summarization performance through lexical overlap between the model-generated and gold summaries. Similarly, the BERTScore is a standard way to compare the semantic similarity between the model-generated and gold summaries by comparing BERT embeddings of both summaries. 
%In summarization tasks, relying solely on lexical similarity might lead to summaries that replicate the source's words but miss its overall meaning. Conversely, only considering semantic similarity might generate summaries that capture the gist but omit specific, crucial details. Using both provides balance.