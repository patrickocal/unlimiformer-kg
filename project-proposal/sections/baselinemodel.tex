\section*{Baseline \texttt{unlimiformer} Model}

Since Vaswani et al 2017, transformers have become the default approach to
natural language processing. Transformers have succeeded due to their ability
to capture long range dependencies between tokens. They do so by abandoning the
sequential approach of recurrent neural networks and instead allowing the
decoder to attend to a complete graph over the encoded hidden states. However,
the complexity of complete graphs is quadratic in the number of tokens and this
explains why LLMs have relatively small context windows. 


To bypass this constraint, numerous creative approaches have been proposed and
most involve breaking the document into chunks of size $k$, where $k$ is equal
to the context window length. Our baseline model \texttt{unlimiformer} (see
figure \ref{fig-latex}) bypasses this constraint by changing the contents of
the context window. That is, instead of passing the next chunk of text in the
sequence, it feeds the decoder the $k$-nearest neighbors that it can find in a
datastore that contains all the tokens in the entire document. Consider a
simplified equation of attention \[\text{Attn}(Q, K, V) = \softmax (Q K^T) V,\]
where, for each hidden-layer state $h_d$ of the decoder and the final-hidden
layer state $h_e$ of the encoder,  $Q = h_d W_Q$, $K = h_e W_K$ and $V = h_e
W_v$.\footnote{In the context of translation, think of $h_d$ as the hidden
  state of tokens in ``I am a student” and $h_e$ as the hidden state of tokens
in ``Je suis \'{e}tudiant’’.} The trick is to rewrite  \[(Q K ^T)_{i, j} =
\langle p_{d,i}, h_{e, j}\rangle\] where $p_{d, i} = h_{d, i} W_Q W_K^T$.
This allows us to create a datastore  $\{h_{e, j} \in \mathcal H_{\text{enc}}
: j \in \text{LongDoc}\}$ and identify the $k$ nearest neighbors in the
datastore to the  projection $p_d$. Only those $k$ nearest neighbors are
passed to the decoder of an otherwise standard LLM.

%Let's consider ``je suis \'{e}tudiant" which has 4 tokens and its translation ``I am a student," which also has 5 tokens. We will walk through the dimensions using these specific lengths for clarity. Assume that we're using a single attention head and a batch size of 1, as you specified.
%
%Notation:
%
%- \( Q \) (Query): Generated from the decoder's hidden state \( h_d \).
%- \( K \) (Key), \( V \) (Value): Generated from the encoder's hidden state \( h_e \).
%- \( d_k \): Dimension of the Key (also the dimension of the Query).
%- \( h_d \) and \( h_e \): Hidden states from the decoder and encoder respectively.
%- \( W_q, W_k \): Weight matrices for Query and Key.
%
%Terms Explained:
%
%1. \( h_e \): The hidden state vector for each word in the input sentence, generated by the encoder. For ``je suis étudiant," the encoder produces 4 hidden state vectors, one for each token.
%
%2. \( Q, K, V \): 
%    - \( Q \) is calculated from the decoder's hidden state \( h_d \) using the weight matrix \( W_q \).
%    - \( K \) and \( V \) are calculated from the encoder's hidden state \( h_e \) using the weight matrices \( W_k \) and \( W_v \).
%
%Dimensionality:
%
%Let's consider \( d_k = 64 \) and \( \text{hidden\_dim} = 128 \) as examples.
%
%1. Hidden State Dimensions:
%    - \( h_d \) for ``I am a student" has dimensions \( (1, 5, 128) \).
%    - \( h_e \) for ``je suis étudiant" has dimensions \( (1, 4, 128) \).
%
%2. Weight Matrix Dimensions:
%    - \( W_q, W_k, W_v \) have dimensions \( (128, 64) \).
%
%3. Calculate \( Q, K, V \):
%    - \( Q = h_d W_q \)  \( (1, 5, 128) \times (128, 64) = (1, 5, 64) \)
%    - \( K = h_e W_k \)  \( (1, 4, 128) \times (128, 64) = (1, 4, 64) \)
%    - \( V = h_e W_v \)  \( (1, 4, 128) \times (128, 64) = (1, 4, 64) \)
%
%4. Calculate \( Q K^T \):
%    - \( Q K^T \) will have dimensions \( (1, 5, 4) \).
%
%5. Scale and Apply Softmax:
%    - \( Q K^T / \sqrt{64} \)  dimension remains \( (1, 5, 4) \)
%    - Softmax applied to last axis: \( \text{softmax}(Q K^T / \sqrt{64}) \)  dimension remains \( (1, 5, 4) \).
%
%6. Calculate \( \text{Attn}(Q, K, V) \):
%    - \( \text{softmax}(Q K^T / \sqrt{64}) \times V \)  \( (1, 5, 4) \times (1, 4, 64) = (1, 5, 64) \)
%
%So, the output \( \text{Attn}(Q, K, V) \) will have dimensions \( (1, 5, 64) \), aligning with the sequence length of the decoder output ``I am a student" and the dimension \( d_k \).
