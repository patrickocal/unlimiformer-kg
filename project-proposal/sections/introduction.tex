\section*{Introduction}
 Augmenting large-language models to allow them to handle large documents using retrieval-based methods is currently a highly active area of research. Many real-world documents are long, with the typical government report being around 10,000 tokens and the average novel being well over 100,000 tokens. The key weakness of modern transformer-based LLMs is the token limit or context window of attention. For instance, the context window of ChatGPT is limited to 4096 tokens.

In this project, we wish to understand how knowledge graphs can improve a state-of-the-art approach to augmenting transformers. The task we perform is the summarization of long documents. Our baseline model is \texttt{unlimiformer} \cite{bertsch2023unlimiformer}, a recent retrieval-based method for augmenting LLMs at the decoder level. The key innovations of \texttt{unlimiformer} are (i) to create a datastore of encodings: one for each token in the original document and (ii) using the $k$-nearest-neighbor algorithm for selecting the $k$ most relevant tokens in the datastore during decoding. We will enrich their datastore with a knowledge graph and replace $k$-NN with graph-based notions of closeness and topology. We will use the GovReport and BookSum datasets, and evaluate performance using the usual ROUGE and BERTScore families of metrics.